<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rd&#39;s F0rest</title>
  
  <subtitle>All about Life and Likes</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://rdf0rest.github.io/"/>
  <updated>2018-08-25T01:13:34.110Z</updated>
  <id>https://rdf0rest.github.io/</id>
  
  <author>
    <name>Tony</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Deep Learning Week 2|Note</title>
    <link href="https://rdf0rest.github.io/2018/08/17/learning2/"/>
    <id>https://rdf0rest.github.io/2018/08/17/learning2/</id>
    <published>2018-08-17T15:36:48.000Z</published>
    <updated>2018-08-25T01:13:34.110Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h2><p>Binary Classification means that output $y$ only have two levels, like 0 and 1.</p><blockquote><p>About the way images stored in computer.<br>Images are stored in computer by <strong>three channels</strong>: Red, Green and Blue.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/RGB.png?raw=true" alt=""><br>As a $64\times 64$ pixels image, there will be three $64 \times 64$ matrices in computer</p></blockquote><p>To turn the RGB matrices into feature vector, we have to unroll three matrices into one vector until we got a long feature vector listing out all the red, green and blue pixel intensity valures.</p><p>We are going to train the Binary Classification model like this:</p><script type="math/tex; mode=display">X(feature \quad vector) \longrightarrow MODEL \longrightarrow Y(label,\quad 0\quad or\quad 1)</script><h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><ul><li>$(x,y)$: single training example<ul><li>$x \in \mathbb{R}^{n_x}$</li><li>$y \in \{0,1\}$</li></ul></li><li>$m$ : number of training examples</li><li>training sets: $\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\dots,(x^{(m)},y^{(m)})\}$</li><li>$X_{m\times n_x} = \begin{bmatrix}<br>\vdots &amp; \vdots &amp;  &amp; \vdots \\<br>x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)}\\<br>\vdots &amp; \vdots &amp;  &amp; \vdots \\<br>\end{bmatrix}$<ul><li>Have to notice that in some algorithms $X$ is stacking up train examples in rows.</li><li>$X \in \mathbb{R}^{n_x \times m}$</li></ul></li><li>$Y = [y^{(1)},y^{(2)},\cdots,y^{(m)}]$<ul><li>$Y \in \mathbb{R}^{1 \times m}$</li></ul></li></ul><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>Supervised Learning, Binary output.</p><script type="math/tex; mode=display">X \longrightarrow Model \longrightarrow \widehat Y = P(Y=1|X)</script><p>Parameters:</p><ul><li>$w\in \mathbb{R}^{n_x}$</li><li>$ b \in \mathbb{R}$: inter-spectrum</li><li>Usually keep $w$ and $b$ separete.</li><li>otherwise, if there is no $b$, like:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/withoutB.png?raw=true" alt=""><br><br><br>For linear Regression:<script type="math/tex; mode=display">\widehat{y} = w^Tx+b \\ 0 \le \widehat{y} \le 1</script><br><br>For Logistic Regression:<script type="math/tex; mode=display">\widehat{y} = \sigma(w^Tx+b)\\\sigma(x) = \frac{1}{1+e^{-x}}</script>where $\sigma(x)$ called <strong>sigmoid function</strong>.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/sigmoidFunction.png?raw=true" alt=""></li></ul><h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>The Logistic Regression Model is persuing to find an efficient relationship between X and Y so we want:</p><script type="math/tex; mode=display">\widehat{Y} = \sigma(w^TX+b) \approx Y</script><h3 id="Loss-error-function"><a href="#Loss-error-function" class="headerlink" title="Loss (error) function:"></a>Loss (error) function:</h3><p>Try to measure how good output $\widehat{y}$ is when the true label is $y$</p><script type="math/tex; mode=display">L(\widehat{y},y) = -(y\log{\widehat{y}}+(1-y)\log{(1-\widehat{y})})</script><p>To demostate that it is getting better when Loss function is getting small, we have:</p><p>\begin{aligned}<br>If y=1: &amp; \quad L(\widehat{y},y)=-\log{\widehat{y}} &amp;\longrightarrow L(\widehat{y},y) smaller, \widehat{y}  larger.\\<br>If y=0: &amp; \quad L(\widehat{y},y)=-\log{(1-\widehat{y})} &amp;\longrightarrow L(\widehat{y},y) larger, \widehat{y}  smaller.\\<br>\end{aligned}</p><p>Notice that the loss function measures what we are doing on <strong>a single example</strong>, While Cost function measure the whole examples:<br>\begin{aligned}<br>J(w,b) =&amp; \frac{1}{m}\sum_{i=1}^m{L(\widehat{y}^{(i)},y^{(i)})}\\<br>=&amp; -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log{\widehat{y}^{(i)}}+(1-y^{(i)})\log{(1-\widehat{y}^{(i)})}]<br>\end{aligned}<br>Fitting model is the way to find parameters whose minimize cost function.</p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Cost function $J(w,b)$ is some surface abov horizontal axes $w$ and $b$ in 3-dimension figure:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/3dcost.png?raw=true" alt=""><br>Cost function is a <strong>convex function</strong>, so we are trying to find the local where $w$ and $b$ minimize enroll cost function. To find the global optimum, there is an algorithm called Gradient Descent.  Here are steps of Gradient Descent algorithm:</p><ul><li>1.Initialize $w$ and $b$ to some initial value;<ul><li>Usually initialize the value to 0 or random initialization. <strong>Due to the function is convex, so any initial point will finally get to the same point.</strong></li></ul></li><li>2.Takes a step in the steepest downhill direction from initial point.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/onestep.png?raw=true" alt=""></li><li>3.Takes some iterations until it get to the global optimum.</li></ul><p><br></p><h3 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h3><p>For some details, we can fix b and have a look at $J(w)$:</p><ul><li>There has an initial point $w_0$</li><li>Repeat:<script type="math/tex; mode=display">w_n := w_{n-1}-\alpha\frac{\mathrm{d}J(w)}{\mathrm{d}w}</script><ul><li>$:=$ means update.</li></ul></li><li>Get to local point.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/cost_details.png?raw=true" alt=""><br><br></li></ul><p>In that iteration there are some notations:</p><ul><li>$\alpha$ <strong>means learning rate</strong> which controls how big a step should be taken on each iteration.</li><li>$\frac{\mathrm{d}J(w)}{\mathrm{d}w}$ represents slope of the function and is basically change or update made to $w_n$<ul><li>When coding, <code>dw</code> will represent this derivative.</li></ul></li></ul><p>So when we apply this to cost function, it becomes:</p><script type="math/tex; mode=display">w := w-\alpha\frac{\partial J(w,b)}{\partial w}\\b := b-\alpha\frac{\partial J(w,b)}{\partial b}</script><h2 id="Computation-Graph-and-It’s-Derivatives"><a href="#Computation-Graph-and-It’s-Derivatives" class="headerlink" title="Computation Graph and It’s Derivatives"></a>Computation Graph and It’s Derivatives</h2><p>Takes $J(a,b,c) = 3(a+bc)$ for an example:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/computationgraph.png?raw=true" alt=""><br>And Now we want to find out $\frac{\mathrm{d} J}{\mathrm{d} v}$, which is obviously equals 3, called a <strong>one step of backpropagation.</strong><br>Consider about $a$. If you are changing $a$ and then that will change $v$. And through chaning $v$, that would change $J$, which in math is <strong>chain rule</strong>:</p><script type="math/tex; mode=display">\frac{\partial J}{\partial a} = \frac{\mathrm{d} J}{\mathrm{d} v}\frac{\mathrm{d} v}{\mathrm{d} a}</script><p>also plays as <strong>second step of backpropagation</strong>.</p><ul><li>while in coding, the derivative below can be represented by <code>dvar</code> and in this case there is <code>da</code><br>So after we work through the Computation graph, we will finish backpropagation computation.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/backpropagation.png?raw=true" alt=""></li></ul><h2 id="Logistic-Regression-Gradient-decent"><a href="#Logistic-Regression-Gradient-decent" class="headerlink" title="Logistic Regression Gradient decent"></a>Logistic Regression Gradient decent</h2><h3 id="For-single-example-with-loss-function"><a href="#For-single-example-with-loss-function" class="headerlink" title="For single example with loss function"></a>For single example with loss function</h3><p>In a Logistic Regression, we need to find $w$ and $b$ to minimize the loss function. The computation graph is like:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/computationgraph2.png?raw=true" alt=""><br>Let’s try to calculate the derivatives of $w$ and $b$:<br>For $a$, there has</p><script type="math/tex; mode=display">\frac{\mathrm{d} L}{\mathrm{d} a} = -\frac{y}{a}+\frac{1-y}{1-a}</script><p>And go for $z$:</p><script type="math/tex; mode=display">\frac{\mathrm{d} L}{\mathrm{d} z} = \frac{\mathrm{d} L}{\mathrm{d} a} \frac{\mathrm{d} a}{\mathrm{d} z} = (-\frac{y}{a}+\frac{1-y}{1-a})\dot [a(1-a)] = a-y</script><p>Finally, we can calculate derivatives of $w$ and $b$:</p><script type="math/tex; mode=display">\frac{\mathrm{d} L}{\mathrm{d} w_1} = x_1(a-y), \quad \frac{\mathrm{d} L}{\mathrm{d} w_2} = x_2(a-y), \quad\frac{\mathrm{d} L}{\mathrm{d} b} = (a-y)</script><h3 id="For-m-examples-with-cost-function"><a href="#For-m-examples-with-cost-function" class="headerlink" title="For $m$ examples with cost function"></a>For $m$ examples with cost function</h3><p>We have know the derivative of $w$ and $b$ and $J(w,b) = \frac{1}{m}\sum_{i=1}^m{L(\widehat{y}^{(i)},y^{(i)})}$, so we can obtain:</p><script type="math/tex; mode=display">\frac{\mathrm{d} J(w,b)}{\mathrm{d} w_1} = \frac{1}{m}\sum_{i=1}^m{\frac{\mathrm{d}}{\mathrm{d} w_1}L(\widehat{y}^{(i)},y^{(i)})}</script><p>which gives us an <strong>overoll gradient</strong>.</p><h2 id="Pesudocode"><a href="#Pesudocode" class="headerlink" title="Pesudocode"></a>Pesudocode</h2><p>For one strp in gradient decent in Logistic Regression:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">J = <span class="number">0</span>; dw_1 = <span class="number">0</span>; dw_2 = <span class="number">0</span>; db = <span class="number">0</span>;</span><br><span class="line">For i <span class="keyword">in</span> <span class="number">1</span> to m:</span><br><span class="line">    zi = wt * xi + b</span><br><span class="line">    ai = sigmoid(zi)</span><br><span class="line">    J += -(yi*log(ai)+(<span class="number">1</span>-yi)*log(<span class="number">1</span>-ai))</span><br><span class="line">    dzi = ai-yi</span><br><span class="line">    dw_1 += xi_1*dzi</span><br><span class="line">    dw_2 += xi_2*dzi</span><br><span class="line">    db   += dzi</span><br><span class="line"></span><br><span class="line">J /= m; dw_1 /= m; dw_2 /= m; db /= m;</span><br><span class="line">w_1 = w_1 - alpha * dw_1</span><br><span class="line">w_2 = w_2 - alpha * dw_2</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure></p><p>Using <strong>vectorization</strong> to get ride of for loop.</p><h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>See the difference between two demos:</p><ul><li><p>Non-vectorized</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(Nx)</span><br><span class="line">    z += w[i] * x[i]</span><br><span class="line">z += b</span><br></pre></td></tr></table></figure></li><li><p>Vectorized</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(w,x) + b</span><br></pre></td></tr></table></figure></li></ul><p><strong>Whenever possible, avoid explicit for-loops.</strong></p><h3 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression:"></a>Vectorizing Logistic Regression:</h3><p>We have know that</p><script type="math/tex; mode=display">X = \begin{bmatrix}\vdots & \vdots &  & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\\vdots & \vdots &  & \vdots \\\end{bmatrix}</script><p>and $z^{(i)} = w^Tx^{(i)}+b$, so for Vectorizing, we can have:</p><script type="math/tex; mode=display">[z^{(1)},z^{(2)},\cdots,z^{(m)}] = w^T \begin{bmatrix}\vdots & \vdots &  & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\\vdots & \vdots &  & \vdots \\\end{bmatrix}+[b,b,\cdots,b] \\\Longrightarrow \qquad Z = w^TX + B</script><p>In python:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(wT,X)+b    <span class="comment">#b is a real number, not a vector</span></span><br></pre></td></tr></table></figure></p><h3 id="Vectorizing-Logistic-Regression’s-Gradient-Output"><a href="#Vectorizing-Logistic-Regression’s-Gradient-Output" class="headerlink" title="Vectorizing Logistic Regression’s Gradient Output"></a>Vectorizing Logistic Regression’s Gradient Output</h3><p>In previous we have obtained that</p><script type="math/tex; mode=display">\frac{\mathrm{d} L}{\mathrm{d} z} = a-y</script><p>So we denote $\frac{\mathrm{d} L}{\mathrm{d} z}$ as $\mathrm{d} z$ and have</p><script type="math/tex; mode=display">\mathrm{d} Z = [\mathrm{d} z^{(1)}, \mathrm{d} z^{(2)}, \cdots, \mathrm{d} z^{(m)}]\\A = [a^{(1)},a^{(2)},\cdots,a^{(m)}], \qquad Y = [y^{(1)},y^{(2)},\cdots,y^{(m)}]\\\mathrm{d} Z = A - Y = [a^{(1)}-y^{(1)},a^{(2)}-y^{(2)},\cdots,a^{(m)}-y^{(m)}]</script><p>The vectorized code will be<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A - Y</span><br><span class="line">dW = <span class="number">1</span>/m * np.dot(X,dZ.T)</span><br><span class="line">db = <span class="number">1</span>/m * np.sum(dZ)</span><br><span class="line"></span><br><span class="line">w = w - alpha * dW</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Binary-Classification&quot;&gt;&lt;a href=&quot;#Binary-Classification&quot; class=&quot;headerlink&quot; title=&quot;Binary Classification&quot;&gt;&lt;/a&gt;Binary Classification&lt;/
      
    
    </summary>
    
      <category term="deeplearning" scheme="https://rdf0rest.github.io/categories/deeplearning/"/>
    
    
      <category term="算法" scheme="https://rdf0rest.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>QAGH Inequality</title>
    <link href="https://rdf0rest.github.io/2018/08/09/QAGH_Inequality/"/>
    <id>https://rdf0rest.github.io/2018/08/09/QAGH_Inequality/</id>
    <published>2018-08-09T07:29:53.000Z</published>
    <updated>2018-08-20T03:00:15.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Proof-of-QAGH-Inequality"><a href="#Proof-of-QAGH-Inequality" class="headerlink" title="Proof of QAGH Inequality"></a>Proof of QAGH Inequality</h1><blockquote><p>The <strong>QM-AM-GM-HM</strong> or <strong>QAGH inequality</strong> generalizes the basic result of the arithmetic mean-geometric mean (AM-GM) inequality, which compares the arithmetic mean (AM) and geometric mean (GM), to include a comparison of the quadratic mean (QM) and harmonic mean (HM).</p></blockquote><ul><li>QM(<strong>Quadratic Mean</strong>):$$Q_n = \sqrt{\frac{\sum_{i=1}^n{{x_i}^2}}{n}} = \sqrt{\frac{x_1^2+x_2^2+\cdots+x_n^2}{n}}$$</li><li>AM(<strong>Arithmetic Mean</strong>):$$A_n = \frac{\sum_{i=1}^n{x_i}}{n}= \frac{x_1+x_2+\cdots+x_n}{n}$$</li><li>GM(<strong>Geometric Mean</strong>):$$G_n = \sqrt[n]{\prod_{i=1}^n{x_i}}=\sqrt[n]{x_1x_2\cdots x_n}$$</li><li>HM(<strong>Harmonic Mean</strong>):$$H_n = \frac{n}{\sum_{i=1}^n{\frac{1}{x_i}}}=\frac{n}{\frac{1}{x_1}+\frac{1}{x_2}+\cdots+\frac{1}{x_n}}$$</li></ul><p>For any list of $n$ nonnegtive real numbers $x_1,x_2,\cdots,x_n$,</p><script type="math/tex; mode=display">H_n \le G_n \le A_n \le Q_n</script><p>and those equalities hold if $x_1 = x_2 = \cdots = x_n$.</p><h2 id="Proofs"><a href="#Proofs" class="headerlink" title="Proofs"></a>Proofs</h2><h3 id="Proofs-of-the-AM–GM-inequality"><a href="#Proofs-of-the-AM–GM-inequality" class="headerlink" title="Proofs of the AM–GM inequality"></a>Proofs of the AM–GM inequality</h3><h4 id="Proofs-by-math-induction"><a href="#Proofs-by-math-induction" class="headerlink" title="Proofs by math induction"></a>Proofs by math induction</h4><p> For $n = 1$, the statement is true with equality.<br> For $n = 2$, there has:</p><script type="math/tex; mode=display"> (x_1-x_2)^2 \ge 0\\ x_1^2 + x_2^2 + 2x_1x_2 - 4x_1x_2 \ge 0\\ (x_1+x_2)^2 \ge 4x_1x_2\\ \frac{x_1+x_2}{2} \ge \sqrt{x_1x_2}</script><p> Suppose that $G_n \le A_n$, there also has $x_{n+1}+(n-1)G_{n+1} \ge n\sqrt[n]{x_{n+1}G_{n+1}^{n-1}}$,<br> Thus:</p><script type="math/tex; mode=display"> \frac{x_1+x_2+\cdots+x_n+x_{n+1}+(n-1)G_{n+1}}{n} \ge G_n+\sqrt[n]{x_{n+1}G_{n+1}^{n-1}} \ge 2\sqrt[2n]{G^n_nx_{n+1}G_{n+1}^{n-1}} = 2G_{n+1}</script><p> Hence</p><script type="math/tex; mode=display">(n+1)A_{n+1} = x_1+x_2+\cdots+x_n+x_{n+1} \ge 2nG_{n+1}-(n-1)G_{n+1} = (n+1)G_{n+1}</script><p> which can obtain</p><script type="math/tex; mode=display">A_{n+1} \ge G_{n+1}</script><p> So the proof has been completed.　　　　　　　　　　　　　　　　　　　　　　　　　$\square$</p><hr><h4 id="Proofs-by-Lagrange-Multipliers"><a href="#Proofs-by-Lagrange-Multipliers" class="headerlink" title="Proofs by Lagrange Multipliers"></a>Proofs by Lagrange Multipliers</h4><p> Suppose that $a_1a_2\cdots a_n=a$ and define the Lagrangian expression as</p><script type="math/tex; mode=display">F = a_1+a_2+\cdots +a_n+\lambda(a_1a_2\cdots a_n-a)</script><p>Let</p><script type="math/tex; mode=display">  \left\{   \begin{aligned}   F^{\prime}_{a_1}&=1+\lambda a_2a_3\cdots a_n = 0,\\   F^{\prime}_{a_2}&=1+\lambda a_1a_3\cdots a_n = 0,\\   & \qquad \quad \cdots \cdots \\   F^{\prime}_{a_n}&=1+\lambda a_1a_2\cdots a_{n-1} = 0,\\   F^{\prime}_{\lambda}&=a_1a_2\cdots a_n-a = 0,   \end{aligned}  \right.</script><p>we have $a_1 = a_2 = \cdots = a_n = \sqrt[n]{a}$.<br>Due to that $a_1,a_2,\cdots,a_n&gt;0$, $a_1+a_2+\cdots+a_n$ only has minimum which is</p><script type="math/tex; mode=display">\min\{a_1+a_2+\cdots+a_n\}=n\sqrt[n]{a}</script><p>imply that $a_1+a_2+\cdots+a_n \ge n\sqrt[n]{a} = n\sqrt[n]{a_1a_2\cdots a_n}$,<br>which completes the proof.　　　　　　　　　　　　　　　　　　　　　　　　　　　　$\square$<br> <br><br><br><br></p><h3 id="Proofs-of-the-GM–HM-inequality"><a href="#Proofs-of-the-GM–HM-inequality" class="headerlink" title="Proofs of the GM–HM inequality"></a>Proofs of the GM–HM inequality</h3><p> For any list of $n$ nonnegtive real numbers $\frac{1}{a_1},\frac{1}{a_2},\cdots,\frac{1}{a_n}$, with $A_{n+1} \ge G_{n+1}$, we have</p><script type="math/tex; mode=display"> \frac{\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}}{n} \ge \sqrt[n]{\frac{1}{a_1}\cdot \frac{1}{a_2}\cdot \cdots \cdot \frac{1}{a_n}} = \frac{1}{\sqrt[n]{a_1a_2\cdots a_n}}</script><p>Taking reciprocal of both sides of the equation, we have</p><script type="math/tex; mode=display">\frac{n}{\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}} \le \sqrt[n]{a_1a_2\cdots a_n}</script><p>which completes the proof.　　　　　　　　　　　　　　　　　　　　　　　　　　　　$\square$<br><br><br><br><br></p><h3 id="Proofs-of-the-QM–AM-inequality"><a href="#Proofs-of-the-QM–AM-inequality" class="headerlink" title="Proofs of the QM–AM inequality"></a>Proofs of the QM–AM inequality</h3><p>Consider that $A_n \le Q_n$ equals $Q_n^2-A_n^2 &gt;0$,<br>so there has<br>\begin{aligned}<br> Q_n^2-A_n^2 = Q_n^2-2A_n^2 + A_n^2 &amp; = \frac{a_1^2+a_2^2+\cdots+a_n^2}{n}-\frac{2a_1A_n+2a_2A_n+\cdots+2a_nA_n}{n} + \frac{nA_n^2}{n}\\<br>&amp; =\frac{(a_1^2-2a_1A_n+A_n^2)+(a_2^2-2a_2A_n+A_n^2)+\cdots+(a_n^2-2a_nA_n+A_n^2)}{n}\\<br>&amp; =\frac{(a_1-A_n)^2+(a_2-A_n)^2+\cdots+(a_n-A_n)^2}{n}\\<br>&amp; \ge 0<br>\end{aligned}<br>This completes the proof.　　　　　　　　　　　　　　　　　　　　　　　　　　　　$\square$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Proof-of-QAGH-Inequality&quot;&gt;&lt;a href=&quot;#Proof-of-QAGH-Inequality&quot; class=&quot;headerlink&quot; title=&quot;Proof of QAGH Inequality&quot;&gt;&lt;/a&gt;Proof of QAGH 
      
    
    </summary>
    
      <category term="Math" scheme="https://rdf0rest.github.io/categories/Math/"/>
    
    
      <category term="数学证明" scheme="https://rdf0rest.github.io/tags/%E6%95%B0%E5%AD%A6%E8%AF%81%E6%98%8E/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning Week 1|Note</title>
    <link href="https://rdf0rest.github.io/2018/08/02/learning/"/>
    <id>https://rdf0rest.github.io/2018/08/02/learning/</id>
    <published>2018-08-02T10:36:48.000Z</published>
    <updated>2018-08-17T15:11:53.036Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Neural-Network"><a href="#What-is-Neural-Network" class="headerlink" title="What is Neural Network?"></a>What is Neural Network?</h2><p>Here is a simple function:</p><script type="math/tex; mode=display">X \longrightarrow \bigcirc \longrightarrow Y</script><p>while in this function, the circle $\bigcirc$ who connects $X$ and $Y$ called “a <strong>Neuron</strong>“.</p><p>And in many ways, a neuron takes max of zero and take off a straight line like:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/A%20neuron.png?raw=true" alt=""><br>This kind of function often be called as “<strong>ReLU</strong>“ function.(<em>rectified linear units</em>). While in a neural networks, it usually consists of lots of single neuron:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/networks.png?raw=true" alt=""></p><p>Due to that each unit of hidden layer connects all input features, so the hidden layer and input layer are <strong>density connected</strong> :</p><ul><li>For each neuron, it accpts all features so that it will decide how many feature are going to use and how to fit the function by itself, so it is density connect.</li></ul><h2 id="Supervised-Learning-with-Neural-Networks"><a href="#Supervised-Learning-with-Neural-Networks" class="headerlink" title="Supervised Learning with Neural Networks"></a>Supervised Learning with Neural Networks</h2><p>Different applications need different kinds of Neural networks.</p><ul><li>Normal Regression: Standard Neural Networks(<strong>SNN</strong>)</li><li>Image recognizing: Convolutional Neural Network(<strong>CNN</strong>)</li><li>Sequence data: Recurrnt Neural Networks(<strong>RNN</strong>)<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/kindsofnetworks.png?raw=true" alt=""><h4 id="Type-of-data"><a href="#Type-of-data" class="headerlink" title="Type of data"></a>Type of data</h4></li><li><p>Structured Data: Basically database of data. Each of the features has a very well defined meaning.</p></li><li><p>Unstructured Data: Refers to things like audio, raw audio, images or text.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;What-is-Neural-Network&quot;&gt;&lt;a href=&quot;#What-is-Neural-Network&quot; class=&quot;headerlink&quot; title=&quot;What is Neural Network?&quot;&gt;&lt;/a&gt;What is Neural Netw
      
    
    </summary>
    
      <category term="deeplearning" scheme="https://rdf0rest.github.io/categories/deeplearning/"/>
    
    
      <category term="算法" scheme="https://rdf0rest.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>hello,2018</title>
    <link href="https://rdf0rest.github.io/2018/06/09/hello-2018/"/>
    <id>https://rdf0rest.github.io/2018/06/09/hello-2018/</id>
    <published>2018-06-09T05:01:00.000Z</published>
    <updated>2018-08-02T05:02:20.999Z</updated>
    
    <content type="html"><![CDATA[<h1 id="你好"><a href="#你好" class="headerlink" title="你好"></a>你好</h1><p>终于在该死的GFM中调试好了自己想要的字体.</p><p>对,就是我最近十分心水的奇葩字体<span style="font-family: OCR A Std">OCR A Std</span>.现在把MAC上所有的IDE的字体都改成了这款,棱角分明,彰显个性.</p><p>特别是那个小小的逗号<span style="font-family: OCR A Std">,</span>,深得我心.</p><p><strong>欢迎来到一个外表普通内心奇葩的代码足球考研狗的内心世界.</strong></p><p>不准骂我XX,除非你也用这种字体<span style="font-family: OCR A Std">SHA BI</span>.</p><h1 id="创始人的话-前言"><a href="#创始人的话-前言" class="headerlink" title="创始人的话|前言"></a>创始人的话|前言</h1><p>虽说整片文章都是创始人我说的话,但是小节的标题并不是一个废话.因为在这里我想作为一个刚刚拥有自己博客的小文青说点东西.</p><p>我之所以能够拥有这一片天地的原因中有很重要的三点:</p><ul><li>真情感谢同性之家Github提供pages,虽然你已经被巨硬收购<span style="font-family: OCR A Std">:&lt; </span></li><li>最近在和老师创建一个R包并将其发布在Github上,就突然想自学如何搭建一个自己的博客</li><li>最主要的原因还是我今日份的考研数学写完了(英语还没看)</li></ul><p>我希望这个博客能够记录我直到30岁之前的人生轨迹.</p><p>如果我能够有幸的成为一个算法工程师的话……</p><p>那就可以说是很幸运了.</p><p>在我敲下键盘的这个时空中,自媒体可以说是风靡我们小学.每每看到身边的小喷友纷纷有了自己的微信公众号,我都羡慕不已.想开,但是惨痛的发现自己写不出任何干货,随罢.发觉小喷友们也写不出啥干货,豁然开朗,重新勾起自媒体的欲望.</p><p>However,我又不大喜欢微信的公众号,觉得 Uh…… 反正就是不喜欢.</p><p>作为互联网时代的原住民,我就不掺和移动互联网时代的原住民的领地了.</p><p>还是喜欢网页的blog,页面宽,适合中老年人放大缩小,可以写干货,可以写代码,有我熟悉的MD(Fxxk GFM),一点点Geek的感觉,不影响边看blog边聊微信,不好看也不用取关我.</p><p>还是希望能够写的好看.</p><p>我会尽力的!谢谢你的支持.</p><h1 id="互联网-代码-Mac"><a href="#互联网-代码-Mac" class="headerlink" title="互联网|代码|Mac"></a>互联网|代码|Mac</h1><p>我是互联网时代的原住民,这话一丁点都没错.</p><p>感觉肉体对自己是一种束缚,(虽然也很有用,特别是用来射门的那两双腿)但是在互联网中,我的意识仿佛可以顺着网络抵达整个地球</p><p>有网络的地方.</p><p>而当中的媒介,正是我手中的这台<strong>MacBook Pro(Retina, 13-inch, Early2015)</strong></p><p>自从我用我的Alienware17换来这台rMBP之后,我就从来没有后悔过.</p><p>不是游戏不好玩了,而是代码和互联网太诱人.就像原始人开始使用工具和西班牙人首次环球航行一样.这种快感像蜘蛛一样将你牢牢控制在蛛网中,不得逃离.</p><p>最开始开始学写代码的时候,感觉自己好像电影里的hacker,对着黑黑的屏幕(后来才直到是命令行)飞速敲下绿绿的代码行(更改IDE颜色),然后整个世界All in control. 然鹅实际开始写代码的时候,才发现自己整天干的就是</p><ul><li>include <stdio.h></stdio.h></li><li>library(ggplot2)</li><li>import scikit-learn</li></ul><p>But,我坚信自己可以成为那种一键回车即可Everything is Done的那种感觉,就像我在男友3,4中的晚年老侦察兵扛着一把M98B顶着几乎为0的K/D到处拿着望远镜到处标记然后一枪打飞头盔.</p><p>这可能就是所谓的厚积薄发吧.</p><p>终于,在我不懈的在stackoverflow上抄来抄去,终于发现了一个可以实现我的愿望的语言:<span style="font-family: OCR A Std">AppleScript</span>在我的最终的调教下,我终于可以实现:</p><p>输入<code>pea</code>来自动连接我的<del>Airpods</del> <del>pee</del> pea.觉得苹果的设计简直良心.</p><p>虽然我的rMBP已经电池循环了近800次了,但是我还是很喜欢它.</p><p>有一天我会用代码控制你的心灵!!!</p><h1 id="Last-but-not-least"><a href="#Last-but-not-least" class="headerlink" title="Last but not least"></a>Last but not least</h1><p>想不到什么好的配图了,就放上这张吧,希望你喜欢.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;你好&quot;&gt;&lt;a href=&quot;#你好&quot; class=&quot;headerlink&quot; title=&quot;你好&quot;&gt;&lt;/a&gt;你好&lt;/h1&gt;&lt;p&gt;终于在该死的GFM中调试好了自己想要的字体.&lt;/p&gt;
&lt;p&gt;对,就是我最近十分心水的奇葩字体&lt;span style=&quot;font-family
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://rdf0rest.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Math</title>
    <link href="https://rdf0rest.github.io/2018/06/09/Math/"/>
    <id>https://rdf0rest.github.io/2018/06/09/Math/</id>
    <published>2018-06-09T04:29:53.000Z</published>
    <updated>2018-08-18T15:04:10.546Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前有提到过，我之所以能够搭建这一片森林的最最最重要的原因是:<strong>那天的数学任务完成了</strong>.</p></blockquote><h1 id="数学任务"><a href="#数学任务" class="headerlink" title="数学任务"></a>数学任务</h1><p>数学可以说是考研狗怎么也迈不过的一道坎，更何况像我这样的考研柯基。<br><img src="https://github.com/RdF0rest/images/blob/master/2018-6-10-math/math-Corgi.jpg?raw=true" alt="NiHao!"></p><p>但是，就如鲁迅所说的:</p><blockquote><p><del>数学是反人类的,是邪教</del> <br>  <del>时间是海绵里的水,挤一挤总会有的</del> <br> <strong>我没说过这句话.</strong></p></blockquote><p>为了保证每天学习数学的目标能够实现,在<strong>女王大人</strong>的要求下(划重点,考试要考),我暂且定下了每天4个小时的数学学习任务.虽然粗略来算,一天足足有6个4小时,But!!!</p><ul><li>睡觉8个小时(考研狗保持充足睡眠)</li><li>写代码3个小时(写代码的时光总是欢乐而短暂的,相对于数学而言)</li><li>上课2小时(如果有课的话)</li><li>卫生间总耗时1小时(人有三急,理解一下)</li><li>中午午休2小时(午觉大于天)</li><li><strong>休息时间2小时</strong></li><li>早晚饭1.5小时(人是铁饭是钢)</li><li>Uh…..</li><li>考研数学!</li></ul><p>这么一算下来,好像每天的时间在纸面上是如此充裕,但是为何在实际生活中却总希望一天有30个小时呢…….</p><p>可能是我沉迷代码的时间太长了?</p><p><strong>屁叫!</strong>湖南的大人肯定会说.</p><p>真的需要加强时间管理了…努力用好omnifocus!</p><p>反正不管怎么说,考研数学对于初期的考研狗来说,便是考研的全部了.而我之所以先选择了线性代数部分复习,一方面是刚开始跟着老师作项目的时候公式推导的矩阵表达竟然看不懂?!为了不被老师发现从而把我踢出幕后小组,我决定从线性代数开始恶补起.</p><h1 id="恶补-amp-完成"><a href="#恶补-amp-完成" class="headerlink" title="恶补&amp;完成"></a>恶补&amp;完成</h1><p>恶补自然是因为学得差.学得差的原因是因为大一的时候没有好好听课.大一没有好好听课的原因我忘了.大一的坑只能用现在的汗水来填了.从三月底开始,之前和女王大人定的时间5月初完成高等代数我竟然生生的拖到了六月份才完成….</p><p>我也是服了我自己.</p><p>想想也是因为没有预想到自己的执行力如此之差加上没想到老师的任务重.不过没有球鞋的痛苦只能我自己忍受.终于拖拖拉拉,在6月份写完了全书高代+高代讲义+分阶习题.唯一让我比较欣慰的是在复习(xuexi)高代的过程中没有任何的敷衍,每个不明白的定理都会自己亲手推出来,所有的错题都会认认真真的用Quiver记下来,回头便于打印重做.也算是踏踏实实的完成了高等代数的学习.</p><p><img src="https://github.com/RdF0rest/images/blob/master/2018-6-10-math/math-quiver.png?raw=true" alt="Alt text" title="Quiver"></p><h1 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h1><p>数学,一个让无数人神魂颠倒的词语,宇宙万物运行的根基.</p><p>数学是美丽的,几乎世间万物都可以溶解成数学的碎片.数学是强大的,人类社会的理性一面由数学一手搭建.数学是深邃的.就像你永远无法知道0和1之间究竟有多少个数字.</p><p>而数学最令我着迷的地方在于,它是人类思想和宇宙意志的链接.蜗居在小小地球上的人类所建立的数学,总能得到广袤宇宙的验证.<strong>这或许就是上帝存在的唯一依据</strong>.</p><p>明天开始就要进入微积分的世界,去亲自领略那触及极限的惊心动魄.</p><p>祝我一帆风顺.</p><p><img src="https://github.com/RdF0rest/images/blob/master/2018-6-10-math/math-equation.png?raw=true" alt=""></p><script type="math/tex; mode=display">y = \frac{10}{x^2}\cos(\frac{100}{x})</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;之前有提到过，我之所以能够搭建这一片森林的最最最重要的原因是:&lt;strong&gt;那天的数学任务完成了&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;数学任务&quot;&gt;&lt;a href=&quot;#数学任务&quot; class=&quot;headerlin
      
    
    </summary>
    
      <category term="Math" scheme="https://rdf0rest.github.io/categories/Math/"/>
    
    
      <category term="考研" scheme="https://rdf0rest.github.io/tags/%E8%80%83%E7%A0%94/"/>
    
  </entry>
  
</feed>
