<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rd&#39;s F0rest</title>
  
  <subtitle>All about Life and Likes</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://rdf0rest.github.io/"/>
  <updated>2020-06-12T10:08:15.704Z</updated>
  <id>https://rdf0rest.github.io/</id>
  
  <author>
    <name>Tony</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>标准化、归一化和中心化</title>
    <link href="https://rdf0rest.github.io/20120/06/12/standard/"/>
    <id>https://rdf0rest.github.io/20120/06/12/standard/</id>
    <published>+020120-06-12T13:40:27.000Z</published>
    <updated>2020-06-12T10:08:15.704Z</updated>
    
    <content type="html"><![CDATA[<h2 id="标准化-Standardizing-Normalization"><a href="#标准化-Standardizing-Normalization" class="headerlink" title="标准化(Standardizing/Normalization)"></a>标准化(Standardizing/Normalization)</h2><p>数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。常用的标准化方法为:</p><script type="math/tex; mode=display">X^* = \frac{X-\mu}{\sigma}</script><p>经过标准化后的数据具有如下的性质:</p><ul><li>$E(X) = 0$</li><li>$D(X) = 1$</li></ul><h2 id="中心化-Zero-centering"><a href="#中心化-Zero-centering" class="headerlink" title="中心化(Zero-centering)"></a>中心化(Zero-centering)</h2><p>数据的中心化是指将数据的中心点(均值)归为0，而对其方差并无要求:</p><script type="math/tex; mode=display">X_{center} = X - \mu</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;标准化-Standardizing-Normalization&quot;&gt;&lt;a href=&quot;#标准化-Standardizing-Normalization&quot; class=&quot;headerlink&quot; title=&quot;标准化(Standardizing/Normalizatio
      
    
    </summary>
    
      <category term="multivariate" scheme="https://rdf0rest.github.io/categories/multivariate/"/>
    
    
      <category term="多元" scheme="https://rdf0rest.github.io/tags/%E5%A4%9A%E5%85%83/"/>
    
  </entry>
  
  <entry>
    <title>Lasso原理以及求解方法</title>
    <link href="https://rdf0rest.github.io/2020/06/11/Lasso/"/>
    <id>https://rdf0rest.github.io/2020/06/11/Lasso/</id>
    <published>2020-06-11T10:36:48.000Z</published>
    <updated>2020-06-12T11:06:08.521Z</updated>
    
    <content type="html"><![CDATA[<h2 id="有偏估计"><a href="#有偏估计" class="headerlink" title="有偏估计"></a>有偏估计</h2><p>Lasso和岭回归的提出是为了解决回归中出现的多重共线性这一问题，即 $\boldsymbol{X}^T\boldsymbol{X}$不满秩的情况。不同于逐步回归，Lasso和岭回归同属于<strong>有偏估计</strong>。</p><p>由OLS估计的正规方程的解可知</p><script type="math/tex; mode=display">\hat{\boldsymbol{\beta}} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}</script><p>当存在多重共线性的时候，会导致$\boldsymbol{X}^T\boldsymbol{X}$难以求逆。因此一个思想就是通过给$\boldsymbol{X}^T\boldsymbol{X}$加一个扰动项（通常为一个正常数矩阵$k\boldsymbol{I}(k&gt;0$）来解决这个问题，这就是有偏估计的思想。在有偏估计下，$\boldsymbol{\beta}$的估计值为</p><script type="math/tex; mode=display">\hat{\boldsymbol{\beta}} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}</script><h2 id="Lasso原理"><a href="#Lasso原理" class="headerlink" title="Lasso原理"></a>Lasso原理</h2><p>Lasso在参数估计的同时既可以对估计值进行压缩，也可以让一些不重要的变量的估计值恰好为0，从而达到变量选择的功能。Lasso回归等价于在OLS回归的基础上给估计值的大小增加一个约束:</p><script type="math/tex; mode=display">\hat{\boldsymbol{\beta}}_{lasso} = \arg \min_{\boldsymbol{\beta}}\{\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p|\beta_j|\}</script><h2 id="Lasso求解：坐标下降算法"><a href="#Lasso求解：坐标下降算法" class="headerlink" title="Lasso求解：坐标下降算法"></a>Lasso求解：坐标下降算法</h2><p>在对数据拟合lasso模型之前，要先对数据进行标准化，消除量纲影响和截距项。经过标准化之后可以得到如下性质:</p><ul><li>$1/n \sum_iy_i = 0$(消去截距项)</li><li>$1/n \sum_ix_{ij} = 0$</li><li>$1/n \sum_ix_{ij}^2 = 1$<br>标准化之后构建目标函数:<script type="math/tex; mode=display">\arg \min_{\boldsymbol{\beta}}\{\frac{1}{2n}\sum_{i=1}^n(y_i-\sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p|\beta_j|\}</script></li></ul><h3 id="一元情况下"><a href="#一元情况下" class="headerlink" title="一元情况下"></a>一元情况下</h3><p>首先考虑只有一个自变量 $x$的情况。对上式进行化简得:</p><script type="math/tex; mode=display">\left\{\begin{aligned}  &const + \frac{1}{2}\beta^2 - \frac{1}{n}\sum_{i=1}^{n}y_ix_i + \lambda\beta, \quad &\beta > 0\\  &const, &\beta = 0\\  &const + \frac{1}{2}\beta^2 - \frac{1}{n}\sum_{i=1}^{n}y_ix_i - \lambda\beta, \quad &\beta < 0\\\end{aligned}\right.</script><p>对 $\beta$求导，得其解析解为</p><script type="math/tex; mode=display">\hat{\beta}  = \left\{\begin{aligned}  &\frac{1}{n}\sum_{i=1}^ny_ix_i - \lambda, \quad &\frac{1}{n}\sum_{i=1}^ny_ix_i > \lambda\\  &0, &other\\  &\frac{1}{n}\sum_{i=1}^ny_ix_i + \lambda, \quad &\frac{1}{n}\sum_{i=1}^ny_ix_i < - \lambda\\\end{aligned}\right.</script><p>可以令 $ \langle x,y\rangle = \sum_{i=1}^ny_ix_i $，定义软阈算子 $S_\lambda(x)$:</p><script type="math/tex; mode=display">S_\lambda(x) = sign(x)(|x| - \lambda)_+</script><p>可得</p><script type="math/tex; mode=display">\hat{\beta} = S_\lambda(\frac{1}{n}<x,y>)</script><p>可以由此求解 $\hat{\beta}$的值。</p><h3 id="多元情况下"><a href="#多元情况下" class="headerlink" title="多元情况下"></a>多元情况下</h3><p>采用坐标循环的方法求解：按照某个固定的顺序重复对变量进行循环，即在第 $j$步时，保持其他的回归系数不变，然后通过最小化目标函数来更新回归系数 $\beta_j$。易知第 $j$步的目标函数是:</p><script type="math/tex; mode=display">\frac{1}{2n}\sum_{i=1}^n(y_i - \sum_{k \ne j}x_{ik}\beta_k - x_{ij}\beta_j)^2 + \lambda \sum_{k \ne j}|\beta_k| + \lambda|\beta_j|</script><p>不妨令偏残差 $r_i^{(j)} = y_i - \sum_{k \ne j} x_{ik}\hat{\beta}_k$，可将其转变成一元的情况，$\hat{\beta}_j$的求解方程为</p><script type="math/tex; mode=display">\hat{\beta}_j = S_\lambda(\frac{1}{n}<x_j, r^{(j)}>)</script><p>通常从一个非常大的 $\lambda$值出发，即 $\lambda_{max} = \max_j|\frac{1}{n}\langle x_j, y \rangle|$,然后逐渐降低 $\lambda$，并且利用前一次所求出的解作为这次求解的热启动。</p><p>由于目标函数是 $\beta$的凸函数，一次可以通过循环坐标下降法收敛到全局最优。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;有偏估计&quot;&gt;&lt;a href=&quot;#有偏估计&quot; class=&quot;headerlink&quot; title=&quot;有偏估计&quot;&gt;&lt;/a&gt;有偏估计&lt;/h2&gt;&lt;p&gt;Lasso和岭回归的提出是为了解决回归中出现的多重共线性这一问题，即 $\boldsymbol{X}^T\boldsymbo
      
    
    </summary>
    
      <category term="deeplearning" scheme="https://rdf0rest.github.io/categories/deeplearning/"/>
    
    
      <category term="算法" scheme="https://rdf0rest.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>距离的度量</title>
    <link href="https://rdf0rest.github.io/2019/04/26/mahalanobis/"/>
    <id>https://rdf0rest.github.io/2019/04/26/mahalanobis/</id>
    <published>2019-04-26T13:40:27.000Z</published>
    <updated>2019-05-05T08:17:36.215Z</updated>
    
    <content type="html"><![CDATA[<h1 id="距离的度量方法"><a href="#距离的度量方法" class="headerlink" title="距离的度量方法"></a>距离的度量方法</h1><h2 id="什么是欧氏距离"><a href="#什么是欧氏距离" class="headerlink" title="什么是欧氏距离"></a>什么是欧氏距离</h2><p>欧几里得度量是指$n$维空间中两个点的真实距离，为</p><script type="math/tex; mode=display">d(x,y)=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}</script><h2 id="什么是马氏距离"><a href="#什么是马氏距离" class="headerlink" title="什么是马氏距离"></a>什么是马氏距离</h2><p>马氏距离是用来表示<strong>数据的协方差距离</strong>。不同于欧氏距离，马氏距离考虑了变量之间的相关关系，并且与尺度无关。<br>对于一个均值为$\boldsymbol{\mu}$，协方差矩阵为$\boldsymbol{\Sigma}$的多元变量$\boldsymbol{x}$，马氏距离有如下表示:</p><script type="math/tex; mode=display">D_M(\boldsymbol{x})=\sqrt{(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}</script><ul><li>如果协方差矩阵为单位矩阵，则马氏距离退化为欧氏距离</li><li>如果协方差矩阵为对角阵，则可称之为正规化的马氏距离</li></ul><h1 id="马氏距离的实际意义"><a href="#马氏距离的实际意义" class="headerlink" title="马氏距离的实际意义"></a>马氏距离的实际意义</h1><h2 id="欧式距离的不足"><a href="#欧式距离的不足" class="headerlink" title="欧式距离的不足"></a>欧式距离的不足</h2><ul><li>当两个变量具有不同的尺度时（比如身高和体重），使用欧氏距离无法得出两个变量是否相似的结论。</li><li>即使是归一化后的欧氏距离，依然无法处理具有分布的变量间距离问题，如下图的$A$点和$B$点<br><img src="https://github.com/RdF0rest/images/blob/master/MultiVariate/M_1.png?raw=true" alt=""><br>在归一化的欧氏距离中，显然这两点距离原点的长度相等。但是由于样本的分布沿着$x$轴分布，因此直观上来看$B$点更倾向于称为样本中的点，而$A$点则可能是离群点。</li><li>如果维度间不独立同分布，即有$y=ax+b$，有<br><img src="https://github.com/RdF0rest/images/blob/master/MultiVariate/M_2.png?raw=true" alt=""><br>在这时由于即使进行数据标准化，也不会改变$A,B$与原点之间距离的<strong>相互关系</strong>。因此我们考虑构建马氏距离。</li></ul><h2 id="马氏距离的几何意义"><a href="#马氏距离的几何意义" class="headerlink" title="马氏距离的几何意义"></a>马氏距离的几何意义</h2><p>马氏距离的构建，是依靠维度间相互独立的基础上进行标准化后的欧氏距离。因此对于变量间不独立同分布的情况下，采用<strong>主成分分析(PCA)</strong>，将变量按照主成分方向旋转使得维度独立，再进行标准化操作。由PCA可知，旋转后的变量方向为特征向量的方向，方差为其对应的特征值。因此得到结果为:<br><img src="https://github.com/RdF0rest/images/blob/master/MultiVariate/M_3.png?raw=true" alt=""><br>可以看出，此时欧氏距离即是我们所求的马氏距离。</p><h2 id="马氏距离的推导"><a href="#马氏距离的推导" class="headerlink" title="马氏距离的推导"></a>马氏距离的推导</h2><p>由主成分分析给出的结果，令 $\boldsymbol{U}=(\boldsymbol{u_1},\boldsymbol{u_2},\dots,\boldsymbol{u_m})$ 为样本<strong>协方差矩阵</strong>的特征向量，$\lambda_1,\lambda_2, \dots, \lambda_m$分别为其对应的特征值，令 $\boldsymbol{F}$ 为根据主成分旋转过后的相互独立的向量，有</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{F}&=(\boldsymbol{F}_1,\boldsymbol{F}_2,\dots,\boldsymbol{F}_m)=\boldsymbol{U}^T\boldsymbol{X}\\\boldsymbol{\mu}_F&=(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\dots,\boldsymbol{\mu}_m)\\(\boldsymbol{F}-\boldsymbol{\mu}_F)&=\boldsymbol{U}^T(\boldsymbol{X}-\boldsymbol{\mu}_X)\end{aligned}</script><p>由主成分分析的性质，变换后的主成分之间相互独立，且方差为特征值，即有</p><script type="math/tex; mode=display">\begin{aligned}\mathrm{Cov}(\boldsymbol{F}-\boldsymbol{\mu}_F)&=\begin{bmatrix}\lambda_1 & & & \\& \lambda_2 & & \\& & \ddots & \\& & & \lambda_m \\\end{bmatrix}\\&=\boldsymbol{U}^T\mathrm{Cov}(\boldsymbol{X}-\boldsymbol{\mu}_X)\boldsymbol{U}\\&=\boldsymbol{U}^T\boldsymbol{\Sigma}_X\boldsymbol{U}\end{aligned}</script><p>马氏距离是在进行主成分旋转过后的欧氏距离，因此马氏距离的计算公式为</p><script type="math/tex; mode=display">\begin{aligned}D_M&=(\frac{f_1-\mu_{F_1}}{\sqrt{\lambda_1}})^2+(\frac{f_2-\mu_{F_2}}{\sqrt{\lambda_2}})^2+\cdots+(\frac{f_m-\mu_{F_m}}{\sqrt{\lambda_m}})^2\\&=(f_1-\mu_{F_1},f_2-\mu_{F_2},\dots,f_m-\mu_{F_m})\begin{pmatrix}\frac{1}{\lambda_1} & & & \\& \frac{1}{\lambda_2} & & \\& & \ddots & \\& & & \frac{1}{\lambda_m}\end{pmatrix}\begin{pmatrix}f_1-\mu_{F_1} \\f_2-\mu_{F_2} \\\vdots\\f_m-\mu_{F_m}\\\end{pmatrix}\\&=(\boldsymbol{F}-\boldsymbol{\mu}_F)^T(\boldsymbol{U}^T\boldsymbol{\Sigma}_X\boldsymbol{U})^{-1}(\boldsymbol{F}-\boldsymbol{\mu}_F)\\&=(\boldsymbol{X}-\boldsymbol{\mu}_X)^T\boldsymbol{\Sigma}_X^{-1}(\boldsymbol{X}-\boldsymbol{\mu}_X)\end{aligned}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;距离的度量方法&quot;&gt;&lt;a href=&quot;#距离的度量方法&quot; class=&quot;headerlink&quot; title=&quot;距离的度量方法&quot;&gt;&lt;/a&gt;距离的度量方法&lt;/h1&gt;&lt;h2 id=&quot;什么是欧氏距离&quot;&gt;&lt;a href=&quot;#什么是欧氏距离&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="multivariate" scheme="https://rdf0rest.github.io/categories/multivariate/"/>
    
    
      <category term="多元" scheme="https://rdf0rest.github.io/tags/%E5%A4%9A%E5%85%83/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning Week 2|Note</title>
    <link href="https://rdf0rest.github.io/2018/08/17/learning2/"/>
    <id>https://rdf0rest.github.io/2018/08/17/learning2/</id>
    <published>2018-08-17T15:36:48.000Z</published>
    <updated>2018-08-25T01:13:34.110Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h2><p>Binary Classification means that output $y$ only have two levels, like 0 and 1.</p><blockquote><p>About the way images stored in computer.<br>Images are stored in computer by <strong>three channels</strong>: Red, Green and Blue.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/RGB.png?raw=true" alt=""><br>As a $64\times 64$ pixels image, there will be three $64 \times 64$ matrices in computer</p></blockquote><p>To turn the RGB matrices into feature vector, we have to unroll three matrices into one vector until we got a long feature vector listing out all the red, green and blue pixel intensity valures.</p><p>We are going to train the Binary Classification model like this:</p><script type="math/tex; mode=display">X(feature \quad vector) \longrightarrow MODEL \longrightarrow Y(label,\quad 0\quad or\quad 1)</script><h3 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h3><ul><li>$(x,y)$: single training example<ul><li>$x \in \mathbb{R}^{n_x}$</li><li>$y \in \{0,1\}$</li></ul></li><li>$m$ : number of training examples</li><li>training sets: $\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\dots,(x^{(m)},y^{(m)})\}$</li><li>$X_{m\times n_x} = \begin{bmatrix}<br>\vdots &amp; \vdots &amp;  &amp; \vdots \\<br>x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)}\\<br>\vdots &amp; \vdots &amp;  &amp; \vdots \\<br>\end{bmatrix}$<ul><li>Have to notice that in some algorithms $X$ is stacking up train examples in rows.</li><li>$X \in \mathbb{R}^{n_x \times m}$</li></ul></li><li>$Y = [y^{(1)},y^{(2)},\cdots,y^{(m)}]$<ul><li>$Y \in \mathbb{R}^{1 \times m}$</li></ul></li></ul><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>Supervised Learning, Binary output.</p><script type="math/tex; mode=display">X \longrightarrow Model \longrightarrow \widehat Y = P(Y=1|X)</script><p>Parameters:</p><ul><li>$w\in \mathbb{R}^{n_x}$</li><li>$ b \in \mathbb{R}$: inter-spectrum</li><li>Usually keep $w$ and $b$ separete.</li><li>otherwise, if there is no $b$, like:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/withoutB.png?raw=true" alt=""><br><br><br>For linear Regression:<script type="math/tex; mode=display">\widehat{y} = w^Tx+b \\ 0 \le \widehat{y} \le 1</script><br><br>For Logistic Regression:<script type="math/tex; mode=display">\widehat{y} = \sigma(w^Tx+b)\\\sigma(x) = \frac{1}{1+e^{-x}}</script>where $\sigma(x)$ called <strong>sigmoid function</strong>.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/sigmoidFunction.png?raw=true" alt=""></li></ul><h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>The Logistic Regression Model is persuing to find an efficient relationship between X and Y so we want:</p><script type="math/tex; mode=display">\widehat{Y} = \sigma(w^TX+b) \approx Y</script><h3 id="Loss-error-function"><a href="#Loss-error-function" class="headerlink" title="Loss (error) function:"></a>Loss (error) function:</h3><p>Try to measure how good output $\widehat{y}$ is when the true label is $y$</p><script type="math/tex; mode=display">L(\widehat{y},y) = -(y\log{\widehat{y}}+(1-y)\log{(1-\widehat{y})})</script><p>To demostate that it is getting better when Loss function is getting small, we have:</p><p>\begin{aligned}<br>If y=1: &amp; \quad L(\widehat{y},y)=-\log{\widehat{y}} &amp;\longrightarrow L(\widehat{y},y) smaller, \widehat{y}  larger.\\<br>If y=0: &amp; \quad L(\widehat{y},y)=-\log{(1-\widehat{y})} &amp;\longrightarrow L(\widehat{y},y) larger, \widehat{y}  smaller.\\<br>\end{aligned}</p><p>Notice that the loss function measures what we are doing on <strong>a single example</strong>, While Cost function measure the whole examples:<br>\begin{aligned}<br>J(w,b) =&amp; \frac{1}{m}\sum_{i=1}^m{L(\widehat{y}^{(i)},y^{(i)})}\\<br>=&amp; -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log{\widehat{y}^{(i)}}+(1-y^{(i)})\log{(1-\widehat{y}^{(i)})}]<br>\end{aligned}<br>Fitting model is the way to find parameters whose minimize cost function.</p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Cost function $J(w,b)$ is some surface abov horizontal axes $w$ and $b$ in 3-dimension figure:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/3dcost.png?raw=true" alt=""><br>Cost function is a <strong>convex function</strong>, so we are trying to find the local where $w$ and $b$ minimize enroll cost function. To find the global optimum, there is an algorithm called Gradient Descent.  Here are steps of Gradient Descent algorithm:</p><ul><li>1.Initialize $w$ and $b$ to some initial value;<ul><li>Usually initialize the value to 0 or random initialization. <strong>Due to the function is convex, so any initial point will finally get to the same point.</strong></li></ul></li><li>2.Takes a step in the steepest downhill direction from initial point.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/onestep.png?raw=true" alt=""></li><li>3.Takes some iterations until it get to the global optimum.</li></ul><p><br></p><h3 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h3><p>For some details, we can fix b and have a look at $J(w)$:</p><ul><li>There has an initial point $w_0$</li><li>Repeat:<script type="math/tex; mode=display">w_n := w_{n-1}-\alpha\frac{\mathrm{d}J(w)}{\mathrm{d}w}</script><ul><li>$:=$ means update.</li></ul></li><li>Get to local point.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/cost_details.png?raw=true" alt=""><br><br></li></ul><p>In that iteration there are some notations:</p><ul><li>$\alpha$ <strong>means learning rate</strong> which controls how big a step should be taken on each iteration.</li><li>$\frac{\mathrm{d}J(w)}{\mathrm{d}w}$ represents slope of the function and is basically change or update made to $w_n$<ul><li>When coding, <code>dw</code> will represent this derivative.</li></ul></li></ul><p>So when we apply this to cost function, it becomes:</p><script type="math/tex; mode=display">w := w-\alpha\frac{\partial J(w,b)}{\partial w}\\b := b-\alpha\frac{\partial J(w,b)}{\partial b}</script><h2 id="Computation-Graph-and-It’s-Derivatives"><a href="#Computation-Graph-and-It’s-Derivatives" class="headerlink" title="Computation Graph and It’s Derivatives"></a>Computation Graph and It’s Derivatives</h2><p>Takes $J(a,b,c) = 3(a+bc)$ for an example:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/computationgraph.png?raw=true" alt=""><br>And Now we want to find out $\frac{\mathrm{d} J}{\mathrm{d} v}$, which is obviously equals 3, called a <strong>one step of backpropagation.</strong><br>Consider about $a$. If you are changing $a$ and then that will change $v$. And through chaning $v$, that would change $J$, which in math is <strong>chain rule</strong>:</p><script type="math/tex; mode=display">\frac{\partial J}{\partial a} = \frac{\mathrm{d} J}{\mathrm{d} v}\frac{\mathrm{d} v}{\mathrm{d} a}</script><p>also plays as <strong>second step of backpropagation</strong>.</p><ul><li>while in coding, the derivative below can be represented by <code>dvar</code> and in this case there is <code>da</code><br>So after we work through the Computation graph, we will finish backpropagation computation.<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/backpropagation.png?raw=true" alt=""></li></ul><h2 id="Logistic-Regression-Gradient-decent"><a href="#Logistic-Regression-Gradient-decent" class="headerlink" title="Logistic Regression Gradient decent"></a>Logistic Regression Gradient decent</h2><h3 id="For-single-example-with-loss-function"><a href="#For-single-example-with-loss-function" class="headerlink" title="For single example with loss function"></a>For single example with loss function</h3><p>In a Logistic Regression, we need to find $w$ and $b$ to minimize the loss function. The computation graph is like:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/computationgraph2.png?raw=true" alt=""><br>Let’s try to calculate the derivatives of $w$ and $b$:<br>For $a$, there has</p><script type="math/tex; mode=display">\frac{\mathrm{d} L}{\mathrm{d} a} = -\frac{y}{a}+\frac{1-y}{1-a}</script><p>And go for $z$:</p><script type="math/tex; mode=display">\frac{\mathrm{d} L}{\mathrm{d} z} = \frac{\mathrm{d} L}{\mathrm{d} a} \frac{\mathrm{d} a}{\mathrm{d} z} = (-\frac{y}{a}+\frac{1-y}{1-a})\dot [a(1-a)] = a-y</script><p>Finally, we can calculate derivatives of $w$ and $b$:</p><script type="math/tex; mode=display">\frac{\mathrm{d} L}{\mathrm{d} w_1} = x_1(a-y), \quad \frac{\mathrm{d} L}{\mathrm{d} w_2} = x_2(a-y), \quad\frac{\mathrm{d} L}{\mathrm{d} b} = (a-y)</script><h3 id="For-m-examples-with-cost-function"><a href="#For-m-examples-with-cost-function" class="headerlink" title="For $m$ examples with cost function"></a>For $m$ examples with cost function</h3><p>We have know the derivative of $w$ and $b$ and $J(w,b) = \frac{1}{m}\sum_{i=1}^m{L(\widehat{y}^{(i)},y^{(i)})}$, so we can obtain:</p><script type="math/tex; mode=display">\frac{\mathrm{d} J(w,b)}{\mathrm{d} w_1} = \frac{1}{m}\sum_{i=1}^m{\frac{\mathrm{d}}{\mathrm{d} w_1}L(\widehat{y}^{(i)},y^{(i)})}</script><p>which gives us an <strong>overoll gradient</strong>.</p><h2 id="Pesudocode"><a href="#Pesudocode" class="headerlink" title="Pesudocode"></a>Pesudocode</h2><p>For one strp in gradient decent in Logistic Regression:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">J = <span class="number">0</span>; dw_1 = <span class="number">0</span>; dw_2 = <span class="number">0</span>; db = <span class="number">0</span>;</span><br><span class="line">For i <span class="keyword">in</span> <span class="number">1</span> to m:</span><br><span class="line">    zi = wt * xi + b</span><br><span class="line">    ai = sigmoid(zi)</span><br><span class="line">    J += -(yi*log(ai)+(<span class="number">1</span>-yi)*log(<span class="number">1</span>-ai))</span><br><span class="line">    dzi = ai-yi</span><br><span class="line">    dw_1 += xi_1*dzi</span><br><span class="line">    dw_2 += xi_2*dzi</span><br><span class="line">    db   += dzi</span><br><span class="line"></span><br><span class="line">J /= m; dw_1 /= m; dw_2 /= m; db /= m;</span><br><span class="line">w_1 = w_1 - alpha * dw_1</span><br><span class="line">w_2 = w_2 - alpha * dw_2</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure></p><p>Using <strong>vectorization</strong> to get ride of for loop.</p><h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>See the difference between two demos:</p><ul><li><p>Non-vectorized</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(Nx)</span><br><span class="line">    z += w[i] * x[i]</span><br><span class="line">z += b</span><br></pre></td></tr></table></figure></li><li><p>Vectorized</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.dot(w,x) + b</span><br></pre></td></tr></table></figure></li></ul><p><strong>Whenever possible, avoid explicit for-loops.</strong></p><h3 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression:"></a>Vectorizing Logistic Regression:</h3><p>We have know that</p><script type="math/tex; mode=display">X = \begin{bmatrix}\vdots & \vdots &  & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\\vdots & \vdots &  & \vdots \\\end{bmatrix}</script><p>and $z^{(i)} = w^Tx^{(i)}+b$, so for Vectorizing, we can have:</p><script type="math/tex; mode=display">[z^{(1)},z^{(2)},\cdots,z^{(m)}] = w^T \begin{bmatrix}\vdots & \vdots &  & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\\vdots & \vdots &  & \vdots \\\end{bmatrix}+[b,b,\cdots,b] \\\Longrightarrow \qquad Z = w^TX + B</script><p>In python:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(wT,X)+b    <span class="comment">#b is a real number, not a vector</span></span><br></pre></td></tr></table></figure></p><h3 id="Vectorizing-Logistic-Regression’s-Gradient-Output"><a href="#Vectorizing-Logistic-Regression’s-Gradient-Output" class="headerlink" title="Vectorizing Logistic Regression’s Gradient Output"></a>Vectorizing Logistic Regression’s Gradient Output</h3><p>In previous we have obtained that</p><script type="math/tex; mode=display">\frac{\mathrm{d} L}{\mathrm{d} z} = a-y</script><p>So we denote $\frac{\mathrm{d} L}{\mathrm{d} z}$ as $\mathrm{d} z$ and have</p><script type="math/tex; mode=display">\mathrm{d} Z = [\mathrm{d} z^{(1)}, \mathrm{d} z^{(2)}, \cdots, \mathrm{d} z^{(m)}]\\A = [a^{(1)},a^{(2)},\cdots,a^{(m)}], \qquad Y = [y^{(1)},y^{(2)},\cdots,y^{(m)}]\\\mathrm{d} Z = A - Y = [a^{(1)}-y^{(1)},a^{(2)}-y^{(2)},\cdots,a^{(m)}-y^{(m)}]</script><p>The vectorized code will be<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A - Y</span><br><span class="line">dW = <span class="number">1</span>/m * np.dot(X,dZ.T)</span><br><span class="line">db = <span class="number">1</span>/m * np.sum(dZ)</span><br><span class="line"></span><br><span class="line">w = w - alpha * dW</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Binary-Classification&quot;&gt;&lt;a href=&quot;#Binary-Classification&quot; class=&quot;headerlink&quot; title=&quot;Binary Classification&quot;&gt;&lt;/a&gt;Binary Classification&lt;/
      
    
    </summary>
    
      <category term="deeplearning" scheme="https://rdf0rest.github.io/categories/deeplearning/"/>
    
    
      <category term="算法" scheme="https://rdf0rest.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>QAGH Inequality</title>
    <link href="https://rdf0rest.github.io/2018/08/09/QAGH_Inequality/"/>
    <id>https://rdf0rest.github.io/2018/08/09/QAGH_Inequality/</id>
    <published>2018-08-09T07:29:53.000Z</published>
    <updated>2018-08-20T03:00:15.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Proof-of-QAGH-Inequality"><a href="#Proof-of-QAGH-Inequality" class="headerlink" title="Proof of QAGH Inequality"></a>Proof of QAGH Inequality</h1><blockquote><p>The <strong>QM-AM-GM-HM</strong> or <strong>QAGH inequality</strong> generalizes the basic result of the arithmetic mean-geometric mean (AM-GM) inequality, which compares the arithmetic mean (AM) and geometric mean (GM), to include a comparison of the quadratic mean (QM) and harmonic mean (HM).</p></blockquote><ul><li>QM(<strong>Quadratic Mean</strong>):$$Q_n = \sqrt{\frac{\sum_{i=1}^n{{x_i}^2}}{n}} = \sqrt{\frac{x_1^2+x_2^2+\cdots+x_n^2}{n}}$$</li><li>AM(<strong>Arithmetic Mean</strong>):$$A_n = \frac{\sum_{i=1}^n{x_i}}{n}= \frac{x_1+x_2+\cdots+x_n}{n}$$</li><li>GM(<strong>Geometric Mean</strong>):$$G_n = \sqrt[n]{\prod_{i=1}^n{x_i}}=\sqrt[n]{x_1x_2\cdots x_n}$$</li><li>HM(<strong>Harmonic Mean</strong>):$$H_n = \frac{n}{\sum_{i=1}^n{\frac{1}{x_i}}}=\frac{n}{\frac{1}{x_1}+\frac{1}{x_2}+\cdots+\frac{1}{x_n}}$$</li></ul><p>For any list of $n$ nonnegtive real numbers $x_1,x_2,\cdots,x_n$,</p><script type="math/tex; mode=display">H_n \le G_n \le A_n \le Q_n</script><p>and those equalities hold if $x_1 = x_2 = \cdots = x_n$.</p><h2 id="Proofs"><a href="#Proofs" class="headerlink" title="Proofs"></a>Proofs</h2><h3 id="Proofs-of-the-AM–GM-inequality"><a href="#Proofs-of-the-AM–GM-inequality" class="headerlink" title="Proofs of the AM–GM inequality"></a>Proofs of the AM–GM inequality</h3><h4 id="Proofs-by-math-induction"><a href="#Proofs-by-math-induction" class="headerlink" title="Proofs by math induction"></a>Proofs by math induction</h4><p> For $n = 1$, the statement is true with equality.<br> For $n = 2$, there has:</p><script type="math/tex; mode=display"> (x_1-x_2)^2 \ge 0\\ x_1^2 + x_2^2 + 2x_1x_2 - 4x_1x_2 \ge 0\\ (x_1+x_2)^2 \ge 4x_1x_2\\ \frac{x_1+x_2}{2} \ge \sqrt{x_1x_2}</script><p> Suppose that $G_n \le A_n$, there also has $x_{n+1}+(n-1)G_{n+1} \ge n\sqrt[n]{x_{n+1}G_{n+1}^{n-1}}$,<br> Thus:</p><script type="math/tex; mode=display"> \frac{x_1+x_2+\cdots+x_n+x_{n+1}+(n-1)G_{n+1}}{n} \ge G_n+\sqrt[n]{x_{n+1}G_{n+1}^{n-1}} \ge 2\sqrt[2n]{G^n_nx_{n+1}G_{n+1}^{n-1}} = 2G_{n+1}</script><p> Hence</p><script type="math/tex; mode=display">(n+1)A_{n+1} = x_1+x_2+\cdots+x_n+x_{n+1} \ge 2nG_{n+1}-(n-1)G_{n+1} = (n+1)G_{n+1}</script><p> which can obtain</p><script type="math/tex; mode=display">A_{n+1} \ge G_{n+1}</script><p> So the proof has been completed.　　　　　　　　　　　　　　　　　　　　　　　　　$\square$</p><hr><h4 id="Proofs-by-Lagrange-Multipliers"><a href="#Proofs-by-Lagrange-Multipliers" class="headerlink" title="Proofs by Lagrange Multipliers"></a>Proofs by Lagrange Multipliers</h4><p> Suppose that $a_1a_2\cdots a_n=a$ and define the Lagrangian expression as</p><script type="math/tex; mode=display">F = a_1+a_2+\cdots +a_n+\lambda(a_1a_2\cdots a_n-a)</script><p>Let</p><script type="math/tex; mode=display">  \left\{   \begin{aligned}   F^{\prime}_{a_1}&=1+\lambda a_2a_3\cdots a_n = 0,\\   F^{\prime}_{a_2}&=1+\lambda a_1a_3\cdots a_n = 0,\\   & \qquad \quad \cdots \cdots \\   F^{\prime}_{a_n}&=1+\lambda a_1a_2\cdots a_{n-1} = 0,\\   F^{\prime}_{\lambda}&=a_1a_2\cdots a_n-a = 0,   \end{aligned}  \right.</script><p>we have $a_1 = a_2 = \cdots = a_n = \sqrt[n]{a}$.<br>Due to that $a_1,a_2,\cdots,a_n&gt;0$, $a_1+a_2+\cdots+a_n$ only has minimum which is</p><script type="math/tex; mode=display">\min\{a_1+a_2+\cdots+a_n\}=n\sqrt[n]{a}</script><p>imply that $a_1+a_2+\cdots+a_n \ge n\sqrt[n]{a} = n\sqrt[n]{a_1a_2\cdots a_n}$,<br>which completes the proof.　　　　　　　　　　　　　　　　　　　　　　　　　　　　$\square$<br> <br><br><br><br></p><h3 id="Proofs-of-the-GM–HM-inequality"><a href="#Proofs-of-the-GM–HM-inequality" class="headerlink" title="Proofs of the GM–HM inequality"></a>Proofs of the GM–HM inequality</h3><p> For any list of $n$ nonnegtive real numbers $\frac{1}{a_1},\frac{1}{a_2},\cdots,\frac{1}{a_n}$, with $A_{n+1} \ge G_{n+1}$, we have</p><script type="math/tex; mode=display"> \frac{\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}}{n} \ge \sqrt[n]{\frac{1}{a_1}\cdot \frac{1}{a_2}\cdot \cdots \cdot \frac{1}{a_n}} = \frac{1}{\sqrt[n]{a_1a_2\cdots a_n}}</script><p>Taking reciprocal of both sides of the equation, we have</p><script type="math/tex; mode=display">\frac{n}{\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}} \le \sqrt[n]{a_1a_2\cdots a_n}</script><p>which completes the proof.　　　　　　　　　　　　　　　　　　　　　　　　　　　　$\square$<br><br><br><br><br></p><h3 id="Proofs-of-the-QM–AM-inequality"><a href="#Proofs-of-the-QM–AM-inequality" class="headerlink" title="Proofs of the QM–AM inequality"></a>Proofs of the QM–AM inequality</h3><p>Consider that $A_n \le Q_n$ equals $Q_n^2-A_n^2 &gt;0$,<br>so there has<br>\begin{aligned}<br> Q_n^2-A_n^2 = Q_n^2-2A_n^2 + A_n^2 &amp; = \frac{a_1^2+a_2^2+\cdots+a_n^2}{n}-\frac{2a_1A_n+2a_2A_n+\cdots+2a_nA_n}{n} + \frac{nA_n^2}{n}\\<br>&amp; =\frac{(a_1^2-2a_1A_n+A_n^2)+(a_2^2-2a_2A_n+A_n^2)+\cdots+(a_n^2-2a_nA_n+A_n^2)}{n}\\<br>&amp; =\frac{(a_1-A_n)^2+(a_2-A_n)^2+\cdots+(a_n-A_n)^2}{n}\\<br>&amp; \ge 0<br>\end{aligned}<br>This completes the proof.　　　　　　　　　　　　　　　　　　　　　　　　　　　　$\square$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Proof-of-QAGH-Inequality&quot;&gt;&lt;a href=&quot;#Proof-of-QAGH-Inequality&quot; class=&quot;headerlink&quot; title=&quot;Proof of QAGH Inequality&quot;&gt;&lt;/a&gt;Proof of QAGH 
      
    
    </summary>
    
      <category term="Math" scheme="https://rdf0rest.github.io/categories/Math/"/>
    
    
      <category term="数学证明" scheme="https://rdf0rest.github.io/tags/%E6%95%B0%E5%AD%A6%E8%AF%81%E6%98%8E/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning Week 1|Note</title>
    <link href="https://rdf0rest.github.io/2018/08/02/learning/"/>
    <id>https://rdf0rest.github.io/2018/08/02/learning/</id>
    <published>2018-08-02T10:36:48.000Z</published>
    <updated>2018-08-17T15:11:53.036Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Neural-Network"><a href="#What-is-Neural-Network" class="headerlink" title="What is Neural Network?"></a>What is Neural Network?</h2><p>Here is a simple function:</p><script type="math/tex; mode=display">X \longrightarrow \bigcirc \longrightarrow Y</script><p>while in this function, the circle $\bigcirc$ who connects $X$ and $Y$ called “a <strong>Neuron</strong>“.</p><p>And in many ways, a neuron takes max of zero and take off a straight line like:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/A%20neuron.png?raw=true" alt=""><br>This kind of function often be called as “<strong>ReLU</strong>“ function.(<em>rectified linear units</em>). While in a neural networks, it usually consists of lots of single neuron:<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/networks.png?raw=true" alt=""></p><p>Due to that each unit of hidden layer connects all input features, so the hidden layer and input layer are <strong>density connected</strong> :</p><ul><li>For each neuron, it accpts all features so that it will decide how many feature are going to use and how to fit the function by itself, so it is density connect.</li></ul><h2 id="Supervised-Learning-with-Neural-Networks"><a href="#Supervised-Learning-with-Neural-Networks" class="headerlink" title="Supervised Learning with Neural Networks"></a>Supervised Learning with Neural Networks</h2><p>Different applications need different kinds of Neural networks.</p><ul><li>Normal Regression: Standard Neural Networks(<strong>SNN</strong>)</li><li>Image recognizing: Convolutional Neural Network(<strong>CNN</strong>)</li><li>Sequence data: Recurrnt Neural Networks(<strong>RNN</strong>)<br><img src="https://github.com/RdF0rest/images/blob/master/DeepLearning/kindsofnetworks.png?raw=true" alt=""><h4 id="Type-of-data"><a href="#Type-of-data" class="headerlink" title="Type of data"></a>Type of data</h4></li><li><p>Structured Data: Basically database of data. Each of the features has a very well defined meaning.</p></li><li><p>Unstructured Data: Refers to things like audio, raw audio, images or text.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;What-is-Neural-Network&quot;&gt;&lt;a href=&quot;#What-is-Neural-Network&quot; class=&quot;headerlink&quot; title=&quot;What is Neural Network?&quot;&gt;&lt;/a&gt;What is Neural Netw
      
    
    </summary>
    
      <category term="deeplearning" scheme="https://rdf0rest.github.io/categories/deeplearning/"/>
    
    
      <category term="算法" scheme="https://rdf0rest.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>hello,2018</title>
    <link href="https://rdf0rest.github.io/2018/06/09/hello-2018/"/>
    <id>https://rdf0rest.github.io/2018/06/09/hello-2018/</id>
    <published>2018-06-09T05:01:00.000Z</published>
    <updated>2018-08-02T05:02:20.999Z</updated>
    
    <content type="html"><![CDATA[<h1 id="你好"><a href="#你好" class="headerlink" title="你好"></a>你好</h1><p>终于在该死的GFM中调试好了自己想要的字体.</p><p>对,就是我最近十分心水的奇葩字体<span style="font-family: OCR A Std">OCR A Std</span>.现在把MAC上所有的IDE的字体都改成了这款,棱角分明,彰显个性.</p><p>特别是那个小小的逗号<span style="font-family: OCR A Std">,</span>,深得我心.</p><p><strong>欢迎来到一个外表普通内心奇葩的代码足球考研狗的内心世界.</strong></p><p>不准骂我XX,除非你也用这种字体<span style="font-family: OCR A Std">SHA BI</span>.</p><h1 id="创始人的话-前言"><a href="#创始人的话-前言" class="headerlink" title="创始人的话|前言"></a>创始人的话|前言</h1><p>虽说整片文章都是创始人我说的话,但是小节的标题并不是一个废话.因为在这里我想作为一个刚刚拥有自己博客的小文青说点东西.</p><p>我之所以能够拥有这一片天地的原因中有很重要的三点:</p><ul><li>真情感谢同性之家Github提供pages,虽然你已经被巨硬收购<span style="font-family: OCR A Std">:&lt; </span></li><li>最近在和老师创建一个R包并将其发布在Github上,就突然想自学如何搭建一个自己的博客</li><li>最主要的原因还是我今日份的考研数学写完了(英语还没看)</li></ul><p>我希望这个博客能够记录我直到30岁之前的人生轨迹.</p><p>如果我能够有幸的成为一个算法工程师的话……</p><p>那就可以说是很幸运了.</p><p>在我敲下键盘的这个时空中,自媒体可以说是风靡我们小学.每每看到身边的小喷友纷纷有了自己的微信公众号,我都羡慕不已.想开,但是惨痛的发现自己写不出任何干货,随罢.发觉小喷友们也写不出啥干货,豁然开朗,重新勾起自媒体的欲望.</p><p>However,我又不大喜欢微信的公众号,觉得 Uh…… 反正就是不喜欢.</p><p>作为互联网时代的原住民,我就不掺和移动互联网时代的原住民的领地了.</p><p>还是喜欢网页的blog,页面宽,适合中老年人放大缩小,可以写干货,可以写代码,有我熟悉的MD(Fxxk GFM),一点点Geek的感觉,不影响边看blog边聊微信,不好看也不用取关我.</p><p>还是希望能够写的好看.</p><p>我会尽力的!谢谢你的支持.</p><h1 id="互联网-代码-Mac"><a href="#互联网-代码-Mac" class="headerlink" title="互联网|代码|Mac"></a>互联网|代码|Mac</h1><p>我是互联网时代的原住民,这话一丁点都没错.</p><p>感觉肉体对自己是一种束缚,(虽然也很有用,特别是用来射门的那两双腿)但是在互联网中,我的意识仿佛可以顺着网络抵达整个地球</p><p>有网络的地方.</p><p>而当中的媒介,正是我手中的这台<strong>MacBook Pro(Retina, 13-inch, Early2015)</strong></p><p>自从我用我的Alienware17换来这台rMBP之后,我就从来没有后悔过.</p><p>不是游戏不好玩了,而是代码和互联网太诱人.就像原始人开始使用工具和西班牙人首次环球航行一样.这种快感像蜘蛛一样将你牢牢控制在蛛网中,不得逃离.</p><p>最开始开始学写代码的时候,感觉自己好像电影里的hacker,对着黑黑的屏幕(后来才直到是命令行)飞速敲下绿绿的代码行(更改IDE颜色),然后整个世界All in control. 然鹅实际开始写代码的时候,才发现自己整天干的就是</p><ul><li>include <stdio.h></stdio.h></li><li>library(ggplot2)</li><li>import scikit-learn</li></ul><p>But,我坚信自己可以成为那种一键回车即可Everything is Done的那种感觉,就像我在男友3,4中的晚年老侦察兵扛着一把M98B顶着几乎为0的K/D到处拿着望远镜到处标记然后一枪打飞头盔.</p><p>这可能就是所谓的厚积薄发吧.</p><p>终于,在我不懈的在stackoverflow上抄来抄去,终于发现了一个可以实现我的愿望的语言:<span style="font-family: OCR A Std">AppleScript</span>在我的最终的调教下,我终于可以实现:</p><p>输入<code>pea</code>来自动连接我的<del>Airpods</del> <del>pee</del> pea.觉得苹果的设计简直良心.</p><p>虽然我的rMBP已经电池循环了近800次了,但是我还是很喜欢它.</p><p>有一天我会用代码控制你的心灵!!!</p><h1 id="Last-but-not-least"><a href="#Last-but-not-least" class="headerlink" title="Last but not least"></a>Last but not least</h1><p>想不到什么好的配图了,就放上这张吧,希望你喜欢.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;你好&quot;&gt;&lt;a href=&quot;#你好&quot; class=&quot;headerlink&quot; title=&quot;你好&quot;&gt;&lt;/a&gt;你好&lt;/h1&gt;&lt;p&gt;终于在该死的GFM中调试好了自己想要的字体.&lt;/p&gt;
&lt;p&gt;对,就是我最近十分心水的奇葩字体&lt;span style=&quot;font-family
      
    
    </summary>
    
    
      <category term="随笔" scheme="https://rdf0rest.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Math</title>
    <link href="https://rdf0rest.github.io/2018/06/09/Math/"/>
    <id>https://rdf0rest.github.io/2018/06/09/Math/</id>
    <published>2018-06-09T04:29:53.000Z</published>
    <updated>2018-08-18T15:04:10.546Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前有提到过，我之所以能够搭建这一片森林的最最最重要的原因是:<strong>那天的数学任务完成了</strong>.</p></blockquote><h1 id="数学任务"><a href="#数学任务" class="headerlink" title="数学任务"></a>数学任务</h1><p>数学可以说是考研狗怎么也迈不过的一道坎，更何况像我这样的考研柯基。<br><img src="https://github.com/RdF0rest/images/blob/master/2018-6-10-math/math-Corgi.jpg?raw=true" alt="NiHao!"></p><p>但是，就如鲁迅所说的:</p><blockquote><p><del>数学是反人类的,是邪教</del> <br>  <del>时间是海绵里的水,挤一挤总会有的</del> <br> <strong>我没说过这句话.</strong></p></blockquote><p>为了保证每天学习数学的目标能够实现,在<strong>女王大人</strong>的要求下(划重点,考试要考),我暂且定下了每天4个小时的数学学习任务.虽然粗略来算,一天足足有6个4小时,But!!!</p><ul><li>睡觉8个小时(考研狗保持充足睡眠)</li><li>写代码3个小时(写代码的时光总是欢乐而短暂的,相对于数学而言)</li><li>上课2小时(如果有课的话)</li><li>卫生间总耗时1小时(人有三急,理解一下)</li><li>中午午休2小时(午觉大于天)</li><li><strong>休息时间2小时</strong></li><li>早晚饭1.5小时(人是铁饭是钢)</li><li>Uh…..</li><li>考研数学!</li></ul><p>这么一算下来,好像每天的时间在纸面上是如此充裕,但是为何在实际生活中却总希望一天有30个小时呢…….</p><p>可能是我沉迷代码的时间太长了?</p><p><strong>屁叫!</strong>湖南的大人肯定会说.</p><p>真的需要加强时间管理了…努力用好omnifocus!</p><p>反正不管怎么说,考研数学对于初期的考研狗来说,便是考研的全部了.而我之所以先选择了线性代数部分复习,一方面是刚开始跟着老师作项目的时候公式推导的矩阵表达竟然看不懂?!为了不被老师发现从而把我踢出幕后小组,我决定从线性代数开始恶补起.</p><h1 id="恶补-amp-完成"><a href="#恶补-amp-完成" class="headerlink" title="恶补&amp;完成"></a>恶补&amp;完成</h1><p>恶补自然是因为学得差.学得差的原因是因为大一的时候没有好好听课.大一没有好好听课的原因我忘了.大一的坑只能用现在的汗水来填了.从三月底开始,之前和女王大人定的时间5月初完成高等代数我竟然生生的拖到了六月份才完成….</p><p>我也是服了我自己.</p><p>想想也是因为没有预想到自己的执行力如此之差加上没想到老师的任务重.不过没有球鞋的痛苦只能我自己忍受.终于拖拖拉拉,在6月份写完了全书高代+高代讲义+分阶习题.唯一让我比较欣慰的是在复习(xuexi)高代的过程中没有任何的敷衍,每个不明白的定理都会自己亲手推出来,所有的错题都会认认真真的用Quiver记下来,回头便于打印重做.也算是踏踏实实的完成了高等代数的学习.</p><p><img src="https://github.com/RdF0rest/images/blob/master/2018-6-10-math/math-quiver.png?raw=true" alt="Alt text" title="Quiver"></p><h1 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h1><p>数学,一个让无数人神魂颠倒的词语,宇宙万物运行的根基.</p><p>数学是美丽的,几乎世间万物都可以溶解成数学的碎片.数学是强大的,人类社会的理性一面由数学一手搭建.数学是深邃的.就像你永远无法知道0和1之间究竟有多少个数字.</p><p>而数学最令我着迷的地方在于,它是人类思想和宇宙意志的链接.蜗居在小小地球上的人类所建立的数学,总能得到广袤宇宙的验证.<strong>这或许就是上帝存在的唯一依据</strong>.</p><p>明天开始就要进入微积分的世界,去亲自领略那触及极限的惊心动魄.</p><p>祝我一帆风顺.</p><p><img src="https://github.com/RdF0rest/images/blob/master/2018-6-10-math/math-equation.png?raw=true" alt=""></p><script type="math/tex; mode=display">y = \frac{10}{x^2}\cos(\frac{100}{x})</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;之前有提到过，我之所以能够搭建这一片森林的最最最重要的原因是:&lt;strong&gt;那天的数学任务完成了&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;数学任务&quot;&gt;&lt;a href=&quot;#数学任务&quot; class=&quot;headerlin
      
    
    </summary>
    
      <category term="Math" scheme="https://rdf0rest.github.io/categories/Math/"/>
    
    
      <category term="考研" scheme="https://rdf0rest.github.io/tags/%E8%80%83%E7%A0%94/"/>
    
  </entry>
  
</feed>
